{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG+V4hhnlJiANQhApLJm5l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WjCBCwC-5ag",
        "outputId": "d2367b77-476a-4218-f820-18b081e8b8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "rf78MqLSRGqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = '/content/drive/MyDrive/data-analytics/uc5'"
      ],
      "metadata": {
        "id": "su8I2QEdRHpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(data_folder, 'Attrition - Data.csv'), header=0, sep=',')\n",
        "df.set_index(\"New_ID\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DRbHB1oRP57",
        "outputId": "bb613ae7-d55d-4974-8f84-8c3cff6f7642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['New_ID', 'Month', 'Txn', 'Status', 'FUA', 'Services', 'Has_Payroll',\n",
            "       'Has_Investment', 'Has_VISA', 'Age', 'Beacon score'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df[df[\"Status\"] == \"Inactive\"]\n",
        "df_2 = df[df[\"Status\"] == \"Closed\"]\n",
        "df_3 = df[df[\"Status\"] == \"Active\"]\n",
        "inActives = pd.unique(df_1[\"New_ID\"])\n",
        "closed = pd.unique(df_2[\"New_ID\"])\n",
        "active = pd.unique(df_3[\"New_ID\"])\n",
        "print(\"Inactive: \", len(inActives))\n",
        "print(\"Closed:\", len(closed))\n",
        "print(\"Active\", len(active))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBp4O6mmRS7g",
        "outputId": "b712c619-59ad-4fb8-9c64-927bca0c38f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inactive:  1575\n",
            "Closed: 51\n",
            "Active 10402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inActives_close = set(active).difference(set(closed))\n",
        "close_inActive = set(active).difference(set(inActives))\n",
        "print(len(inActives_close))\n",
        "print(len(close_inActive))\n",
        "ins = set(inActives).intersection(set(closed))\n",
        "print(\"Inactive turns to closed:\", len(ins))\n",
        "print(\"Total Months\", len(df[\"Month\"].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETMn-xqfRi_Y",
        "outputId": "067de31b-f655-443a-f6a6-cfeb80607d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10351\n",
            "8827\n",
            "Inactive turns to closed: 0\n",
            "Total Months 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def refill_1_and_drop_other(customer_df):\n",
        "    if customer_df['FUA'].isnull().sum() > 0:\n",
        "        customer_df = customer_df.fillna(method='ffill', limit=1)\n",
        "        customer_df = customer_df.dropna()\n",
        "    return customer_df"
      ],
      "metadata": {
        "id": "KMVGXe33ShqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index(\"New_ID\")\n",
        "print(len(df.index))\n",
        "print(df['Status'].value_counts())\n",
        "\n",
        "grouped = df.groupby('New_ID')\n",
        "\n",
        "cleaned_df = pd.concat([refill_1_and_drop_other(group) for _, group in grouped])\n",
        "\n",
        "cleaned_df.to_csv(os.path.join(data_folder, 'clean_up.csv'), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1f_lLJQSvBf",
        "outputId": "533d3f5d-98e7-45ec-9f61-0c7940adcee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "499296\n",
            "Active      469893\n",
            "Inactive     28350\n",
            "Closed        1053\n",
            "Name: Status, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = cleaned_df\n",
        "df['Has_Payroll'] = df['Has_Payroll'].replace('Yes', 1)\n",
        "df['Has_Payroll'] = df['Has_Payroll'].replace('yes', 1)\n",
        "df['Has_Payroll'] = df['Has_Payroll'].replace('No', 0)\n",
        "df['Age'] = df['Age']/100\n",
        "df['FUA'] = np.nan_to_num(np.log10(df['FUA']+1e-9), nan=-1)\n",
        "df['FUA'] = (df['FUA'] - df['FUA'].min())/(df['FUA'].max() - df['FUA'].min())\n",
        "df['Beacon score'] = (df['Beacon score'] - df['Beacon score'].min())/(df['Beacon score'].max() -\n",
        "                                                                      df['Beacon score'].min())\n",
        "df['Services'] = df['Services']/10\n",
        "df['Txn'] = df['Txn']/100\n",
        "df.loc[(df['Txn'] == 0) & (df['Status'] == 'Active'), 'Status'] = 'Churning'\n",
        "df['target'] = df['Status']\n",
        "df['target'] = df['target'].replace('Active', 0)\n",
        "df['target'] = df['target'].replace('Churning', 1)\n",
        "df['target'] = df['target'].replace('Inactive', 2)\n",
        "df['target'] = df['target'].replace('Closed', 3)\n",
        "df.to_csv(os.path.join(data_folder, 'preprocessed.csv'), index=False)"
      ],
      "metadata": {
        "id": "Uce-0CdZT22C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "S62j99hIT4dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tagger = Pre_trained()\n",
        "        self.linear = nn.Linear(in_features=48, out_features=4)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.tagger(input)\n",
        "        batchsize, _, _ = output.shape\n",
        "        output = output.contiguous().view(batchsize, -1)\n",
        "        predict = self.linear(output)\n",
        "        return predict"
      ],
      "metadata": {
        "id": "Mg8oaQaVUckO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index][0].astype(np.float32)\n",
        "        label = self.data[index][1]\n",
        "        return torch.tensor(data), torch.tensor(label)"
      ],
      "metadata": {
        "id": "w9wAfA9EUf1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset_pred(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index][0].astype(np.float32)\n",
        "        id = self.data[index][1]  # str\n",
        "        return torch.tensor(data), id"
      ],
      "metadata": {
        "id": "tKNR00aSUrDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainloader, optimizer, criterion, epoch):\n",
        "    device = \"cuda\"\n",
        "    model.train()\n",
        "    for param in model.tagger.parameters():\n",
        "        if epoch < 2:\n",
        "            param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True\n",
        "    tp, tn, fp, fn, train_loss = 0, 0, 0, 0, 0\n",
        "    accept, refuse = 0, 0\n",
        "    for idx, data in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        input, gt_label = data\n",
        "        input.to(device)\n",
        "        # gt_label = F.one_hot(gt_label.to(torch.int64), num_classes=4)\n",
        "        gt_label.to(device)\n",
        "        predict = model(input)\n",
        "\n",
        "        loss = criterion(predict, gt_label)\n",
        "        predicted = torch.argmax(predict, dim=1).detach().cpu().numpy()\n",
        "        labelled = gt_label.detach().cpu().numpy()\n",
        "\n",
        "        tp += (np.sum((predicted == labelled) & (predicted != 0)))\n",
        "        tn += (np.sum((predicted == labelled) & (predicted == 0)))\n",
        "        fp += (np.sum((predicted != 0) & (labelled == 0)))\n",
        "        fn += (np.sum((predicted == 0) & (labelled != 0)))\n",
        "        train_loss += loss.item() * len(data)\n",
        "\n",
        "        accept += (np.sum((predicted == labelled)))\n",
        "        refuse += (np.sum((predicted != labelled)))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    acc = accept / (accept + refuse)\n",
        "    return train_loss, acc"
      ],
      "metadata": {
        "id": "vPAkRm_NUtMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, val_dataloader, criterion):\n",
        "    device = \"cuda\"\n",
        "    model.eval()\n",
        "    tp, tn, fp, fn, val_loss = 0, 0, 0, 0, 0\n",
        "    accept, refuse = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(val_dataloader):\n",
        "            input, gt_label = data\n",
        "            input.to(device)\n",
        "            # gt_label = F.one_hot(gt_label.to(torch.int64), num_classes=4)\n",
        "            gt_label.to(device)\n",
        "            predict = model(input)\n",
        "\n",
        "            loss = criterion(predict, gt_label)\n",
        "            predicted = torch.argmax(predict, dim=1).detach().cpu().numpy()\n",
        "            labelled = gt_label.detach().cpu().numpy()\n",
        "\n",
        "            tp += (np.sum((predicted == labelled) & (predicted != 0)))\n",
        "            tn += (np.sum((predicted == labelled) & (predicted == 0)))\n",
        "            fp += (np.sum((predicted != 0) & (labelled == 0)))\n",
        "            fn += (np.sum((predicted == 0) & (labelled != 0)))\n",
        "            val_loss += loss.item() * len(data)\n",
        "\n",
        "            accept += (np.sum((predicted == labelled)))\n",
        "            refuse += (np.sum((predicted != labelled)))\n",
        "        acc = accept / (accept + refuse)\n",
        "    return val_loss, acc"
      ],
      "metadata": {
        "id": "P3Mu5onHUxNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, predict_dataloader):\n",
        "    model.eval()\n",
        "    df = pd.DataFrame(columns=['New_ID', 'Pred_Status', 'Pred_P_Active', 'Pred_P_Churning',\n",
        "                               'Pred_P_Inactive', 'Pred_P_Closed'])\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(predict_dataloader):\n",
        "            input, id = data\n",
        "            input.to(device)\n",
        "            # gt_label = F.one_hot(gt_label.to(torch.int64), num_classes=4)\n",
        "            predict = model(input)\n",
        "            pred_label = torch.argmax(predict, dim=1).detach().cpu().numpy()[0]\n",
        "            pred = F.softmax(predict, dim=1)\n",
        "            pred = pred[0].detach().cpu().numpy()\n",
        "            df.loc[idx] = [id, pred_label, pred[0], pred[1], pred[2], pred[3]]\n",
        "    return df"
      ],
      "metadata": {
        "id": "E6bsPP3VUz-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCH = 10\n",
        "TRAIN = True\n",
        "\n",
        "df = pd.read_csv(os.path.join(data_folder, 'preprocessed.csv'), header=0, sep=',')\n",
        "selected_columns = ['Txn', 'FUA', 'Services', 'Has_Payroll', 'Has_Investment', 'Has_VISA', 'Age', 'Beacon score']\n",
        "\n",
        "all_pred = []  # to predict\n",
        "pred_by_last_status = [[], [], []]  # to predict  0 for active, 1 for churn, 2 for inactive, 202112\n",
        "\n",
        "train_data = []\n",
        "train_label = []\n",
        "for customer, customer_df in df.groupby('New_ID'):\n",
        "    if (customer_df['Status'] == 'Closed').sum() > 0:\n",
        "        if len(customer_df.index) > 12:\n",
        "            data_df = customer_df[selected_columns]\n",
        "            slices = int(np.floor(len(customer_df.index) / 12))\n",
        "            remove = len(customer_df.index) % 12\n",
        "            if remove == 0:\n",
        "                remove = 12\n",
        "                slices = slices - 1\n",
        "            data = data_df.values[remove-1:]\n",
        "            data = data[:-1].reshape(slices, 12, 8)\n",
        "            label_df = customer_df[['target']]\n",
        "            label = label_df.values[remove:].ravel().reshape(slices, 12)\n",
        "            for i in range(slices):\n",
        "                train_data.append(data[i])\n",
        "                train_label.append(label[i][11])\n",
        "    else:\n",
        "        data_df = customer_df[selected_columns]\n",
        "        data = data_df.values.reshape(4, 12, 8)\n",
        "        label_df = customer_df[['target']]\n",
        "        label = label_df.values.ravel().reshape(4, 12)\n",
        "        for i in range(3):\n",
        "            train_data.append(data[i])\n",
        "            train_label.append(label[i+1][0])\n",
        "        all_pred.append([data[3], customer])\n",
        "        pred_by_last_status[label[3][11]].append([data[3], customer])\n",
        "\n",
        "prepare_dataset = []\n",
        "for i in range(len(train_data)):\n",
        "    prepare_dataset.append([train_data[i], train_label[i]])\n",
        "\n",
        "train_val_set, test_set = train_test_split(prepare_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
        "train_set, val_set = train_test_split(train_val_set, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "train_dataset = MyDataset(train_set)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "val_dataset = MyDataset(val_set)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "model = Predictor()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.8)\n",
        "\n",
        "if TRAIN:\n",
        "    # train\n",
        "    # pretrained_model = torch.load(os.path.join(data_folder, \"learnModel.pth\"))\n",
        "    # model.tagger.load_state_dict(pretrained_model)\n",
        "    for epoch in range(EPOCH):\n",
        "        for param in model.tagger.parameters():\n",
        "            if epoch < 2:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "        train_loss, train_acc = train_model(model, train_loader, optimizer, criterion, epoch)\n",
        "        val_loss, val_acc = eval_model(model, val_loader, criterion)\n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        print(\"Train: loss\", train_loss / len(train_set), \" acc: \", train_acc)\n",
        "        print(\"Validate: loss\", val_loss / len(val_set), \" acc: \", val_acc)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(data_folder, \"2_churnModel.pth\"))\n",
        "\n",
        "    # test\n",
        "    test_dataset = MyDataset(test_set)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
        "\n",
        "    test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
        "    print(\"\\nTest: loss\", test_loss / len(val_set), \" acc: \", test_acc)\n",
        "else:\n",
        "    weights = torch.load(os.path.join(data_folder, \"2_churnModel.pth\"))\n",
        "    model.load_state_dict(weights)\n",
        "\n",
        "# predict\n",
        "pred_dataset = MyDataset_pred(all_pred)\n",
        "pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
        "\n",
        "pred_df = predict(model, pred_loader)\n",
        "pred_df.to_csv(os.path.join(data_folder, \"churn2_predict.csv\"), index=False)\n",
        "\n",
        "# predict based on last status\n",
        "for i in range(3):\n",
        "    pred_dataset = MyDataset_pred(pred_by_last_status[i])\n",
        "    pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
        "\n",
        "    pred_df = predict(model, pred_loader)\n",
        "    file_name = \"churn2_predict_\"+str(i+1)+\".csv\"\n",
        "    pred_df.to_csv(os.path.join(data_folder, file_name), index=False)"
      ],
      "metadata": {
        "id": "eWFaueYMU3FL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}